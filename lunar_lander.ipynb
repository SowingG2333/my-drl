{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc51ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mlp network for lunarlander\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import os\n",
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "def layer_init(layer: nn.Module, std: float=np.sqrt(2), bias: float=0.01) -> nn.Module:\n",
    "    # initialize the linear/convolutional layer\n",
    "    if hasattr(layer, \"weight\") and layer.weight is not None:\n",
    "        nn.init.orthogonal_(layer.weight, gain=std)\n",
    "    if hasattr(layer, \"bias\") and layer.bias is not None:\n",
    "        nn.init.constant_(layer.bias, bias)\n",
    "    return layer\n",
    "\n",
    "class LunarLanderMLP(nn.Module):\n",
    "    # initialize the mlp\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        # define the critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(8, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0)\n",
    "        )\n",
    "        # define the actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(8, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01)\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x) # [batch, num_actions]\n",
    "        dist = Categorical(logits=logits) # [batch, num_actions] after softmax to construct a distribution\n",
    "        if action is None: # output actions in sample stage\n",
    "            action = dist.sample() # tips: tensor.sample() is wrong!!!\n",
    "        log_prob = dist.log_prob(action) # [batch]\n",
    "        entropy = dist.entropy() # [batch]\n",
    "        value = self.critic(x).squeeze(-1) # [batch]\n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81b431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_gae(\n",
    "    rewards: torch.Tensor, \n",
    "    dones: torch.Tensor, \n",
    "    values: torch.Tensor, \n",
    "    next_value: torch.Tensor, \n",
    "    next_done: torch.Tensor, \n",
    "    gamma: float, \n",
    "    gae_lambda: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # calculate GAE: general advantage estimination\n",
    "    # need: next_obs(next_value, next_done), rewards, dones, values\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    next_advantage = 0\n",
    "    num_steps = rewards.shape[0]\n",
    "    for t in reversed(range(num_steps)):\n",
    "        # if num_step == 100, t = 99, 98 ... 0\n",
    "        # if done == 0, mask == 1, game didn't stop\n",
    "        # next_done and next_value is the current state (internal vars)\n",
    "        # check if it is the last step\n",
    "        if t == num_steps - 1:\n",
    "            mask = 1.0 - next_done\n",
    "            next_value = next_value \n",
    "        else:\n",
    "            # if it is not the last step, check the next step in the buffer\n",
    "            mask = 1.0 - dones[t] # tips: to match the input dones, we should use dones[t] instead pf dones[t+1]\n",
    "            next_value = values[t + 1]\n",
    "        # calculate the td_error/delta\n",
    "        delta = rewards[t] + mask * gamma * next_value - values[t]\n",
    "        # calculate the GAE\n",
    "        current_advantage = delta + gamma * gae_lambda * mask * next_advantage\n",
    "        advantages[t] = current_advantage # save current advantage\n",
    "        next_advantage = current_advantage\n",
    "    returns = advantages + values\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b8fe23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LunarLander envs are created! obs_shape:(8,); action_n:4\n"
     ]
    }
   ],
   "source": [
    "# define hyper-params\n",
    "env_id = \"LunarLander-v3\"\n",
    "num_envs = 32\n",
    "capture_video = False\n",
    "run_name = \"exp_1\"  # extracted as a variable for consistency\n",
    "\n",
    "# environment factory function\n",
    "def make_env(env_id: str, idx: int, capture_video: bool, run_name: str) -> Callable[[], gym.Env]:\n",
    "    def thunk() -> gym.Env:\n",
    "        # create the basic env\n",
    "        if capture_video and idx == 0: # only capture the env_id == 0\n",
    "            env = gym.make(env_id, render_mode='rgb_array')\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        # add a wrapper to get the real rewards\n",
    "        # what is a wrapper in gym?\n",
    "        # wrapper can add functions for the env without modifying the source code\n",
    "        # e.g., output more infos after env.step(action) or record videos\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "# create the vector envs\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, i, capture_video, run_name) for i in range(num_envs)]\n",
    ")   \n",
    "\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete)\n",
    "print(\n",
    "    f\"LunarLander envs are created! \"\n",
    "    f\"obs_shape:{envs.single_observation_space.shape}; \"\n",
    "    f\"action_n:{envs.single_action_space.n}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dee718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "Update 1/152 | step=65536 | ep_ret_mean=-184.6 ep_len_mean=91.4 | loss=286.192 pg=-0.021 v=572.452 ent=1.379 | clipfrac=0.025 exp_var=-0.007\n",
      "Update 2/152 | step=131072 | ep_ret_mean=-156.0 ep_len_mean=96.8 | loss=188.643 pg=-0.096 v=377.507 ent=1.357 | clipfrac=0.042 exp_var=-0.011\n",
      "Update 3/152 | step=196608 | ep_ret_mean=-143.6 ep_len_mean=106.0 | loss=134.204 pg=0.027 v=268.380 ent=1.325 | clipfrac=0.053 exp_var=-0.001\n",
      "Update 4/152 | step=262144 | ep_ret_mean=-132.5 ep_len_mean=115.3 | loss=112.786 pg=-0.067 v=225.731 ent=1.281 | clipfrac=0.059 exp_var=-0.000\n",
      "Update 5/152 | step=327680 | ep_ret_mean=-103.1 ep_len_mean=132.3 | loss=103.987 pg=-0.005 v=208.009 ent=1.227 | clipfrac=0.106 exp_var=-0.000\n",
      "Update 6/152 | step=393216 | ep_ret_mean=-80.7 ep_len_mean=147.9 | loss=101.929 pg=0.008 v=203.866 ent=1.179 | clipfrac=0.064 exp_var=-0.002\n",
      "Update 7/152 | step=458752 | ep_ret_mean=-49.7 ep_len_mean=181.3 | loss=79.546 pg=0.004 v=159.107 ent=1.133 | clipfrac=0.051 exp_var=0.010\n",
      "Update 8/152 | step=524288 | ep_ret_mean=-55.3 ep_len_mean=227.9 | loss=86.674 pg=0.073 v=173.224 ent=1.107 | clipfrac=0.038 exp_var=0.014\n",
      "Update 9/152 | step=589824 | ep_ret_mean=-37.6 ep_len_mean=312.6 | loss=61.557 pg=-0.010 v=123.155 ent=1.106 | clipfrac=0.029 exp_var=0.036\n",
      "Update 10/152 | step=655360 | ep_ret_mean=-33.6 ep_len_mean=405.4 | loss=47.474 pg=0.016 v=94.938 ent=1.094 | clipfrac=0.039 exp_var=0.059\n",
      "Update 11/152 | step=720896 | ep_ret_mean=-20.8 ep_len_mean=564.6 | loss=39.900 pg=0.011 v=79.799 ent=1.069 | clipfrac=0.037 exp_var=0.103\n",
      "Update 12/152 | step=786432 | ep_ret_mean=-21.3 ep_len_mean=677.9 | loss=26.213 pg=-0.017 v=52.480 ent=1.081 | clipfrac=0.051 exp_var=0.096\n",
      "Update 13/152 | step=851968 | ep_ret_mean=-9.5 ep_len_mean=822.0 | loss=16.253 pg=-0.016 v=32.558 ent=1.046 | clipfrac=0.035 exp_var=0.123\n",
      "Update 14/152 | step=917504 | ep_ret_mean=0.1 ep_len_mean=921.8 | loss=16.768 pg=0.046 v=33.464 ent=0.986 | clipfrac=0.062 exp_var=0.182\n",
      "Update 15/152 | step=983040 | ep_ret_mean=-15.2 ep_len_mean=965.9 | loss=10.523 pg=0.018 v=21.030 ent=0.973 | clipfrac=0.054 exp_var=0.086\n",
      "Update 16/152 | step=1048576 | ep_ret_mean=-48.6 ep_len_mean=946.7 | loss=10.317 pg=-0.002 v=20.656 ent=0.912 | clipfrac=0.052 exp_var=0.034\n",
      "Update 17/152 | step=1114112 | ep_ret_mean=-73.8 ep_len_mean=990.1 | loss=9.085 pg=0.038 v=18.113 ent=0.962 | clipfrac=0.050 exp_var=-0.017\n",
      "Update 18/152 | step=1179648 | ep_ret_mean=-58.5 ep_len_mean=994.9 | loss=6.320 pg=0.018 v=12.623 ent=0.933 | clipfrac=0.032 exp_var=0.000\n",
      "Update 19/152 | step=1245184 | ep_ret_mean=-52.9 ep_len_mean=991.7 | loss=5.821 pg=-0.051 v=11.762 ent=0.914 | clipfrac=0.033 exp_var=0.002\n",
      "Update 20/152 | step=1310720 | ep_ret_mean=-53.1 ep_len_mean=997.7 | loss=4.382 pg=-0.004 v=8.790 ent=0.871 | clipfrac=0.032 exp_var=0.000\n",
      "Update 21/152 | step=1376256 | ep_ret_mean=-55.0 ep_len_mean=1000.0 | loss=3.560 pg=0.105 v=6.929 ent=0.891 | clipfrac=0.020 exp_var=0.002\n",
      "Update 22/152 | step=1441792 | ep_ret_mean=-63.4 ep_len_mean=1000.0 | loss=3.283 pg=0.037 v=6.509 ent=0.864 | clipfrac=0.019 exp_var=0.000\n",
      "Update 23/152 | step=1507328 | ep_ret_mean=-69.4 ep_len_mean=1000.0 | loss=3.855 pg=0.075 v=7.579 ent=0.875 | clipfrac=0.021 exp_var=-0.000\n",
      "Update 24/152 | step=1572864 | ep_ret_mean=-71.1 ep_len_mean=1000.0 | loss=3.130 pg=-0.025 v=6.328 ent=0.882 | clipfrac=0.016 exp_var=0.000\n",
      "Update 25/152 | step=1638400 | ep_ret_mean=-76.4 ep_len_mean=999.0 | loss=3.458 pg=-0.025 v=6.985 ent=0.878 | clipfrac=0.015 exp_var=0.000\n",
      "Update 26/152 | step=1703936 | ep_ret_mean=-77.8 ep_len_mean=1000.0 | loss=3.425 pg=0.021 v=6.824 ent=0.834 | clipfrac=0.025 exp_var=0.000\n",
      "Update 27/152 | step=1769472 | ep_ret_mean=-80.1 ep_len_mean=1000.0 | loss=3.239 pg=-0.054 v=6.603 ent=0.867 | clipfrac=0.020 exp_var=-0.000\n",
      "Update 28/152 | step=1835008 | ep_ret_mean=-78.9 ep_len_mean=1000.0 | loss=3.248 pg=0.021 v=6.471 ent=0.879 | clipfrac=0.013 exp_var=-0.000\n",
      "Update 29/152 | step=1900544 | ep_ret_mean=-86.4 ep_len_mean=999.3 | loss=3.076 pg=-0.060 v=6.289 ent=0.844 | clipfrac=0.014 exp_var=-0.000\n",
      "Update 30/152 | step=1966080 | ep_ret_mean=-83.7 ep_len_mean=993.2 | loss=2.720 pg=0.034 v=5.389 ent=0.848 | clipfrac=0.018 exp_var=0.000\n",
      "Update 31/152 | step=2031616 | ep_ret_mean=-89.6 ep_len_mean=1000.0 | loss=3.976 pg=-0.016 v=8.001 ent=0.853 | clipfrac=0.020 exp_var=0.000\n",
      "Update 32/152 | step=2097152 | ep_ret_mean=-81.2 ep_len_mean=1000.0 | loss=3.056 pg=0.068 v=5.993 ent=0.861 | clipfrac=0.021 exp_var=0.000\n",
      "Update 33/152 | step=2162688 | ep_ret_mean=-88.2 ep_len_mean=997.7 | loss=2.726 pg=0.047 v=5.376 ent=0.873 | clipfrac=0.021 exp_var=-0.000\n",
      "Update 34/152 | step=2228224 | ep_ret_mean=-84.5 ep_len_mean=1000.0 | loss=2.797 pg=0.051 v=5.509 ent=0.817 | clipfrac=0.015 exp_var=0.000\n",
      "Update 35/152 | step=2293760 | ep_ret_mean=-80.2 ep_len_mean=994.5 | loss=2.964 pg=0.027 v=5.890 ent=0.820 | clipfrac=0.021 exp_var=-0.000\n",
      "Update 36/152 | step=2359296 | ep_ret_mean=-87.0 ep_len_mean=1000.0 | loss=2.998 pg=-0.075 v=6.162 ent=0.834 | clipfrac=0.016 exp_var=0.000\n",
      "Update 37/152 | step=2424832 | ep_ret_mean=-80.8 ep_len_mean=1000.0 | loss=2.566 pg=0.070 v=5.011 ent=0.877 | clipfrac=0.017 exp_var=0.000\n",
      "Update 38/152 | step=2490368 | ep_ret_mean=-82.5 ep_len_mean=1000.0 | loss=3.289 pg=0.085 v=6.424 ent=0.807 | clipfrac=0.013 exp_var=0.000\n",
      "Update 39/152 | step=2555904 | ep_ret_mean=-83.4 ep_len_mean=1000.0 | loss=2.866 pg=-0.015 v=5.778 ent=0.754 | clipfrac=0.016 exp_var=0.000\n",
      "Update 40/152 | step=2621440 | ep_ret_mean=-87.5 ep_len_mean=996.2 | loss=5.705 pg=0.079 v=11.270 ent=0.874 | clipfrac=0.017 exp_var=0.000\n",
      "Update 41/152 | step=2686976 | ep_ret_mean=-84.8 ep_len_mean=1000.0 | loss=3.662 pg=-0.058 v=7.457 ent=0.794 | clipfrac=0.022 exp_var=0.000\n",
      "Update 42/152 | step=2752512 | ep_ret_mean=-80.8 ep_len_mean=995.0 | loss=3.302 pg=-0.050 v=6.721 ent=0.788 | clipfrac=0.022 exp_var=-0.000\n",
      "Update 43/152 | step=2818048 | ep_ret_mean=-80.9 ep_len_mean=1000.0 | loss=2.916 pg=-0.006 v=5.860 ent=0.835 | clipfrac=0.015 exp_var=-0.000\n",
      "Update 44/152 | step=2883584 | ep_ret_mean=-77.6 ep_len_mean=1000.0 | loss=3.269 pg=-0.061 v=6.676 ent=0.801 | clipfrac=0.013 exp_var=0.000\n",
      "Update 45/152 | step=2949120 | ep_ret_mean=-73.7 ep_len_mean=988.6 | loss=3.351 pg=0.028 v=6.662 ent=0.784 | clipfrac=0.016 exp_var=0.000\n",
      "Update 46/152 | step=3014656 | ep_ret_mean=-80.6 ep_len_mean=992.8 | loss=6.157 pg=-0.008 v=12.346 ent=0.819 | clipfrac=0.018 exp_var=-0.000\n",
      "Update 47/152 | step=3080192 | ep_ret_mean=-73.6 ep_len_mean=1000.0 | loss=3.115 pg=0.026 v=6.195 ent=0.823 | clipfrac=0.015 exp_var=0.000\n",
      "Update 48/152 | step=3145728 | ep_ret_mean=-77.3 ep_len_mean=1000.0 | loss=3.215 pg=0.043 v=6.361 ent=0.798 | clipfrac=0.017 exp_var=-0.000\n",
      "Update 49/152 | step=3211264 | ep_ret_mean=-71.9 ep_len_mean=1000.0 | loss=3.261 pg=-0.031 v=6.598 ent=0.749 | clipfrac=0.016 exp_var=0.000\n",
      "Update 50/152 | step=3276800 | ep_ret_mean=-73.7 ep_len_mean=995.6 | loss=3.106 pg=-0.033 v=6.293 ent=0.779 | clipfrac=0.018 exp_var=-0.003\n",
      "Update 51/152 | step=3342336 | ep_ret_mean=-72.3 ep_len_mean=979.9 | loss=3.610 pg=-0.032 v=7.299 ent=0.779 | clipfrac=0.020 exp_var=0.000\n",
      "Update 52/152 | step=3407872 | ep_ret_mean=-71.9 ep_len_mean=1000.0 | loss=2.204 pg=0.008 v=4.406 ent=0.764 | clipfrac=0.019 exp_var=0.000\n",
      "Update 53/152 | step=3473408 | ep_ret_mean=-73.5 ep_len_mean=988.3 | loss=2.624 pg=-0.003 v=5.269 ent=0.781 | clipfrac=0.019 exp_var=-0.001\n",
      "Update 54/152 | step=3538944 | ep_ret_mean=-74.3 ep_len_mean=998.4 | loss=2.577 pg=-0.142 v=5.453 ent=0.769 | clipfrac=0.016 exp_var=0.032\n",
      "Update 55/152 | step=3604480 | ep_ret_mean=-76.3 ep_len_mean=981.9 | loss=3.857 pg=0.042 v=7.644 ent=0.777 | clipfrac=0.022 exp_var=0.171\n",
      "Update 56/152 | step=3670016 | ep_ret_mean=-79.9 ep_len_mean=996.6 | loss=3.144 pg=-0.028 v=6.360 ent=0.787 | clipfrac=0.020 exp_var=0.355\n",
      "Update 57/152 | step=3735552 | ep_ret_mean=-75.9 ep_len_mean=996.2 | loss=1.696 pg=0.003 v=3.401 ent=0.782 | clipfrac=0.024 exp_var=0.513\n",
      "Update 58/152 | step=3801088 | ep_ret_mean=-74.0 ep_len_mean=978.1 | loss=3.041 pg=0.058 v=5.981 ent=0.770 | clipfrac=0.020 exp_var=0.562\n",
      "Update 59/152 | step=3866624 | ep_ret_mean=-75.9 ep_len_mean=987.7 | loss=2.243 pg=0.047 v=4.407 ent=0.795 | clipfrac=0.022 exp_var=0.656\n",
      "Update 60/152 | step=3932160 | ep_ret_mean=-73.1 ep_len_mean=981.8 | loss=1.417 pg=-0.048 v=2.947 ent=0.817 | clipfrac=0.028 exp_var=0.673\n",
      "Update 61/152 | step=3997696 | ep_ret_mean=-69.1 ep_len_mean=1000.0 | loss=1.248 pg=-0.034 v=2.579 ent=0.747 | clipfrac=0.021 exp_var=0.763\n",
      "Update 62/152 | step=4063232 | ep_ret_mean=-69.0 ep_len_mean=1000.0 | loss=1.442 pg=-0.025 v=2.949 ent=0.788 | clipfrac=0.026 exp_var=0.783\n",
      "Update 63/152 | step=4128768 | ep_ret_mean=-69.0 ep_len_mean=1000.0 | loss=1.318 pg=-0.076 v=2.802 ent=0.744 | clipfrac=0.032 exp_var=0.806\n",
      "Update 64/152 | step=4194304 | ep_ret_mean=-67.5 ep_len_mean=991.8 | loss=2.112 pg=-0.030 v=4.301 ent=0.806 | clipfrac=0.037 exp_var=0.754\n",
      "Update 65/152 | step=4259840 | ep_ret_mean=-66.8 ep_len_mean=992.9 | loss=1.084 pg=-0.025 v=2.233 ent=0.751 | clipfrac=0.035 exp_var=0.781\n",
      "Update 66/152 | step=4325376 | ep_ret_mean=-63.6 ep_len_mean=986.4 | loss=0.908 pg=-0.038 v=1.907 ent=0.762 | clipfrac=0.038 exp_var=0.759\n",
      "Update 67/152 | step=4390912 | ep_ret_mean=-58.5 ep_len_mean=987.5 | loss=0.994 pg=-0.053 v=2.110 ent=0.769 | clipfrac=0.042 exp_var=0.806\n",
      "Update 68/152 | step=4456448 | ep_ret_mean=-57.9 ep_len_mean=991.7 | loss=0.867 pg=0.012 v=1.725 ent=0.764 | clipfrac=0.034 exp_var=0.775\n",
      "Update 69/152 | step=4521984 | ep_ret_mean=-57.0 ep_len_mean=990.8 | loss=1.080 pg=-0.050 v=2.275 ent=0.741 | clipfrac=0.033 exp_var=0.745\n",
      "Update 70/152 | step=4587520 | ep_ret_mean=-57.1 ep_len_mean=996.2 | loss=1.453 pg=0.068 v=2.786 ent=0.756 | clipfrac=0.028 exp_var=0.795\n",
      "Update 71/152 | step=4653056 | ep_ret_mean=-48.7 ep_len_mean=989.8 | loss=1.302 pg=0.012 v=2.597 ent=0.790 | clipfrac=0.034 exp_var=0.828\n",
      "Update 72/152 | step=4718592 | ep_ret_mean=-49.4 ep_len_mean=976.4 | loss=0.866 pg=-0.022 v=1.791 ent=0.724 | clipfrac=0.034 exp_var=0.760\n",
      "Update 73/152 | step=4784128 | ep_ret_mean=-52.5 ep_len_mean=999.4 | loss=1.036 pg=0.003 v=2.081 ent=0.717 | clipfrac=0.021 exp_var=0.766\n",
      "Update 74/152 | step=4849664 | ep_ret_mean=-47.7 ep_len_mean=976.9 | loss=1.837 pg=-0.047 v=3.782 ent=0.725 | clipfrac=0.023 exp_var=0.641\n",
      "Update 75/152 | step=4915200 | ep_ret_mean=-55.3 ep_len_mean=975.9 | loss=2.941 pg=0.009 v=5.878 ent=0.747 | clipfrac=0.026 exp_var=0.642\n",
      "Update 76/152 | step=4980736 | ep_ret_mean=-48.6 ep_len_mean=995.8 | loss=12.580 pg=0.138 v=24.898 ent=0.748 | clipfrac=0.027 exp_var=0.731\n",
      "Update 77/152 | step=5046272 | ep_ret_mean=-45.5 ep_len_mean=982.7 | loss=4.760 pg=0.043 v=9.449 ent=0.764 | clipfrac=0.034 exp_var=0.696\n",
      "Update 78/152 | step=5111808 | ep_ret_mean=-46.3 ep_len_mean=972.3 | loss=5.283 pg=0.051 v=10.480 ent=0.753 | clipfrac=0.024 exp_var=0.669\n",
      "Update 79/152 | step=5177344 | ep_ret_mean=-44.6 ep_len_mean=979.0 | loss=9.046 pg=0.084 v=17.939 ent=0.766 | clipfrac=0.020 exp_var=0.635\n",
      "Update 80/152 | step=5242880 | ep_ret_mean=-38.2 ep_len_mean=979.7 | loss=2.360 pg=-0.010 v=4.756 ent=0.726 | clipfrac=0.027 exp_var=0.620\n",
      "Update 81/152 | step=5308416 | ep_ret_mean=-36.7 ep_len_mean=998.0 | loss=1.687 pg=-0.011 v=3.413 ent=0.785 | clipfrac=0.022 exp_var=0.770\n",
      "Update 82/152 | step=5373952 | ep_ret_mean=-47.3 ep_len_mean=968.9 | loss=4.303 pg=-0.024 v=8.669 ent=0.720 | clipfrac=0.019 exp_var=0.562\n",
      "Update 83/152 | step=5439488 | ep_ret_mean=-23.1 ep_len_mean=990.6 | loss=1.134 pg=0.027 v=2.230 ent=0.700 | clipfrac=0.026 exp_var=0.696\n",
      "Update 84/152 | step=5505024 | ep_ret_mean=-19.7 ep_len_mean=980.3 | loss=2.934 pg=-0.023 v=5.929 ent=0.728 | clipfrac=0.019 exp_var=0.617\n",
      "Update 85/152 | step=5570560 | ep_ret_mean=-35.3 ep_len_mean=973.7 | loss=5.140 pg=0.013 v=10.268 ent=0.743 | clipfrac=0.021 exp_var=0.604\n",
      "Update 86/152 | step=5636096 | ep_ret_mean=-16.1 ep_len_mean=980.4 | loss=10.837 pg=0.025 v=21.639 ent=0.746 | clipfrac=0.022 exp_var=0.555\n",
      "Update 87/152 | step=5701632 | ep_ret_mean=-20.4 ep_len_mean=981.8 | loss=5.758 pg=-0.033 v=11.597 ent=0.751 | clipfrac=0.021 exp_var=0.597\n",
      "Update 88/152 | step=5767168 | ep_ret_mean=-19.1 ep_len_mean=968.1 | loss=9.135 pg=0.011 v=18.260 ent=0.690 | clipfrac=0.030 exp_var=0.597\n",
      "Update 89/152 | step=5832704 | ep_ret_mean=-10.7 ep_len_mean=960.6 | loss=11.502 pg=0.004 v=23.010 ent=0.714 | clipfrac=0.025 exp_var=0.657\n",
      "Update 90/152 | step=5898240 | ep_ret_mean=-7.3 ep_len_mean=960.1 | loss=7.454 pg=0.066 v=14.790 ent=0.706 | clipfrac=0.031 exp_var=0.694\n",
      "Update 91/152 | step=5963776 | ep_ret_mean=-18.8 ep_len_mean=965.9 | loss=11.269 pg=-0.010 v=22.573 ent=0.726 | clipfrac=0.032 exp_var=0.614\n",
      "Update 92/152 | step=6029312 | ep_ret_mean=-10.4 ep_len_mean=984.1 | loss=6.712 pg=-0.056 v=13.551 ent=0.715 | clipfrac=0.030 exp_var=0.648\n",
      "Update 93/152 | step=6094848 | ep_ret_mean=-14.6 ep_len_mean=956.9 | loss=8.212 pg=-0.022 v=16.483 ent=0.762 | clipfrac=0.026 exp_var=0.644\n",
      "Update 94/152 | step=6160384 | ep_ret_mean=-3.6 ep_len_mean=949.3 | loss=21.740 pg=0.070 v=43.356 ent=0.747 | clipfrac=0.021 exp_var=0.637\n",
      "Update 95/152 | step=6225920 | ep_ret_mean=5.3 ep_len_mean=960.6 | loss=24.571 pg=0.037 v=49.084 ent=0.765 | clipfrac=0.027 exp_var=0.673\n",
      "Update 96/152 | step=6291456 | ep_ret_mean=7.8 ep_len_mean=935.3 | loss=15.090 pg=0.031 v=30.132 ent=0.725 | clipfrac=0.033 exp_var=0.644\n",
      "Update 97/152 | step=6356992 | ep_ret_mean=5.5 ep_len_mean=942.3 | loss=4.736 pg=-0.011 v=9.508 ent=0.746 | clipfrac=0.030 exp_var=0.631\n",
      "Update 98/152 | step=6422528 | ep_ret_mean=8.1 ep_len_mean=940.4 | loss=4.466 pg=-0.006 v=8.958 ent=0.747 | clipfrac=0.043 exp_var=0.629\n",
      "Update 99/152 | step=6488064 | ep_ret_mean=32.2 ep_len_mean=946.1 | loss=21.570 pg=0.112 v=42.932 ent=0.753 | clipfrac=0.022 exp_var=0.691\n",
      "Update 100/152 | step=6553600 | ep_ret_mean=49.8 ep_len_mean=917.2 | loss=10.594 pg=-0.045 v=21.293 ent=0.738 | clipfrac=0.041 exp_var=0.699\n",
      "Update 101/152 | step=6619136 | ep_ret_mean=34.6 ep_len_mean=914.1 | loss=15.946 pg=0.006 v=31.895 ent=0.777 | clipfrac=0.048 exp_var=0.677\n",
      "Update 102/152 | step=6684672 | ep_ret_mean=32.8 ep_len_mean=867.2 | loss=11.312 pg=0.068 v=22.502 ent=0.724 | clipfrac=0.036 exp_var=0.672\n",
      "Update 103/152 | step=6750208 | ep_ret_mean=44.8 ep_len_mean=878.1 | loss=8.635 pg=-0.019 v=17.322 ent=0.736 | clipfrac=0.037 exp_var=0.681\n",
      "Update 104/152 | step=6815744 | ep_ret_mean=54.8 ep_len_mean=897.8 | loss=11.673 pg=0.028 v=23.305 ent=0.732 | clipfrac=0.032 exp_var=0.739\n",
      "Update 105/152 | step=6881280 | ep_ret_mean=66.4 ep_len_mean=886.2 | loss=6.898 pg=-0.010 v=13.830 ent=0.762 | clipfrac=0.027 exp_var=0.726\n",
      "Update 106/152 | step=6946816 | ep_ret_mean=78.6 ep_len_mean=850.4 | loss=14.753 pg=-0.058 v=29.638 ent=0.726 | clipfrac=0.045 exp_var=0.761\n",
      "Update 107/152 | step=7012352 | ep_ret_mean=60.0 ep_len_mean=902.8 | loss=7.221 pg=0.054 v=14.350 ent=0.751 | clipfrac=0.037 exp_var=0.759\n",
      "Update 108/152 | step=7077888 | ep_ret_mean=63.1 ep_len_mean=849.3 | loss=25.278 pg=0.061 v=50.451 ent=0.780 | clipfrac=0.042 exp_var=0.739\n",
      "Update 109/152 | step=7143424 | ep_ret_mean=56.1 ep_len_mean=877.8 | loss=6.748 pg=-0.006 v=13.522 ent=0.739 | clipfrac=0.037 exp_var=0.748\n",
      "Update 110/152 | step=7208960 | ep_ret_mean=81.3 ep_len_mean=820.9 | loss=25.144 pg=-0.001 v=50.305 ent=0.740 | clipfrac=0.042 exp_var=0.734\n",
      "Update 111/152 | step=7274496 | ep_ret_mean=64.6 ep_len_mean=905.3 | loss=6.080 pg=-0.020 v=12.216 ent=0.802 | clipfrac=0.030 exp_var=0.742\n",
      "Update 112/152 | step=7340032 | ep_ret_mean=81.7 ep_len_mean=848.0 | loss=19.320 pg=-0.023 v=38.703 ent=0.776 | clipfrac=0.035 exp_var=0.734\n",
      "Update 113/152 | step=7405568 | ep_ret_mean=79.2 ep_len_mean=891.2 | loss=7.862 pg=0.023 v=15.693 ent=0.769 | clipfrac=0.039 exp_var=0.784\n",
      "Update 114/152 | step=7471104 | ep_ret_mean=69.5 ep_len_mean=839.0 | loss=13.906 pg=0.096 v=27.635 ent=0.776 | clipfrac=0.028 exp_var=0.687\n",
      "Update 115/152 | step=7536640 | ep_ret_mean=97.6 ep_len_mean=855.3 | loss=8.046 pg=-0.035 v=16.177 ent=0.764 | clipfrac=0.035 exp_var=0.801\n",
      "Update 116/152 | step=7602176 | ep_ret_mean=89.6 ep_len_mean=856.2 | loss=7.629 pg=-0.022 v=15.316 ent=0.725 | clipfrac=0.043 exp_var=0.749\n",
      "Update 117/152 | step=7667712 | ep_ret_mean=85.2 ep_len_mean=859.1 | loss=3.966 pg=-0.032 v=8.012 ent=0.772 | clipfrac=0.027 exp_var=0.778\n",
      "Update 118/152 | step=7733248 | ep_ret_mean=101.1 ep_len_mean=823.6 | loss=7.632 pg=-0.001 v=15.280 ent=0.727 | clipfrac=0.026 exp_var=0.757\n",
      "Update 119/152 | step=7798784 | ep_ret_mean=97.9 ep_len_mean=837.1 | loss=11.045 pg=0.034 v=22.039 ent=0.777 | clipfrac=0.034 exp_var=0.780\n",
      "Update 120/152 | step=7864320 | ep_ret_mean=96.4 ep_len_mean=797.9 | loss=13.404 pg=-0.012 v=26.847 ent=0.704 | clipfrac=0.031 exp_var=0.753\n",
      "Update 121/152 | step=7929856 | ep_ret_mean=95.2 ep_len_mean=790.2 | loss=8.346 pg=-0.024 v=16.754 ent=0.742 | clipfrac=0.034 exp_var=0.691\n",
      "Update 122/152 | step=7995392 | ep_ret_mean=97.3 ep_len_mean=829.3 | loss=13.674 pg=0.047 v=27.270 ent=0.777 | clipfrac=0.037 exp_var=0.786\n",
      "Update 123/152 | step=8060928 | ep_ret_mean=100.0 ep_len_mean=857.7 | loss=2.566 pg=0.010 v=5.129 ent=0.780 | clipfrac=0.027 exp_var=0.819\n",
      "Update 124/152 | step=8126464 | ep_ret_mean=111.5 ep_len_mean=811.3 | loss=8.821 pg=-0.032 v=17.722 ent=0.763 | clipfrac=0.024 exp_var=0.823\n",
      "Update 125/152 | step=8192000 | ep_ret_mean=112.6 ep_len_mean=790.7 | loss=7.590 pg=0.027 v=15.141 ent=0.749 | clipfrac=0.034 exp_var=0.808\n",
      "Update 126/152 | step=8257536 | ep_ret_mean=99.9 ep_len_mean=831.9 | loss=4.552 pg=0.069 v=8.980 ent=0.744 | clipfrac=0.033 exp_var=0.744\n",
      "Update 127/152 | step=8323072 | ep_ret_mean=108.6 ep_len_mean=851.2 | loss=5.436 pg=-0.042 v=10.971 ent=0.742 | clipfrac=0.042 exp_var=0.837\n",
      "Update 128/152 | step=8388608 | ep_ret_mean=106.9 ep_len_mean=839.2 | loss=4.596 pg=0.079 v=9.050 ent=0.760 | clipfrac=0.027 exp_var=0.821\n",
      "Update 129/152 | step=8454144 | ep_ret_mean=115.6 ep_len_mean=824.6 | loss=13.001 pg=0.005 v=26.006 ent=0.726 | clipfrac=0.039 exp_var=0.817\n",
      "Update 130/152 | step=8519680 | ep_ret_mean=121.6 ep_len_mean=832.3 | loss=4.206 pg=-0.007 v=8.440 ent=0.751 | clipfrac=0.033 exp_var=0.859\n",
      "Update 131/152 | step=8585216 | ep_ret_mean=122.8 ep_len_mean=847.4 | loss=4.315 pg=0.005 v=8.636 ent=0.749 | clipfrac=0.032 exp_var=0.885\n",
      "Update 132/152 | step=8650752 | ep_ret_mean=112.8 ep_len_mean=865.7 | loss=4.634 pg=-0.075 v=9.434 ent=0.719 | clipfrac=0.033 exp_var=0.862\n",
      "Update 133/152 | step=8716288 | ep_ret_mean=124.8 ep_len_mean=815.1 | loss=4.580 pg=0.065 v=9.044 ent=0.727 | clipfrac=0.040 exp_var=0.859\n",
      "Update 134/152 | step=8781824 | ep_ret_mean=127.6 ep_len_mean=844.2 | loss=11.033 pg=-0.009 v=22.099 ent=0.712 | clipfrac=0.041 exp_var=0.901\n",
      "Update 135/152 | step=8847360 | ep_ret_mean=127.0 ep_len_mean=846.0 | loss=4.408 pg=-0.047 v=8.924 ent=0.714 | clipfrac=0.043 exp_var=0.875\n",
      "Update 136/152 | step=8912896 | ep_ret_mean=126.3 ep_len_mean=849.0 | loss=2.338 pg=0.019 v=4.652 ent=0.697 | clipfrac=0.064 exp_var=0.881\n",
      "Update 137/152 | step=8978432 | ep_ret_mean=126.2 ep_len_mean=869.5 | loss=1.452 pg=0.033 v=2.851 ent=0.666 | clipfrac=0.058 exp_var=0.865\n",
      "Update 138/152 | step=9043968 | ep_ret_mean=139.0 ep_len_mean=836.8 | loss=2.031 pg=0.004 v=4.067 ent=0.619 | clipfrac=0.080 exp_var=0.854\n",
      "Update 139/152 | step=9109504 | ep_ret_mean=165.3 ep_len_mean=762.4 | loss=7.539 pg=0.017 v=15.055 ent=0.615 | clipfrac=0.114 exp_var=0.730\n",
      "Update 140/152 | step=9175040 | ep_ret_mean=180.2 ep_len_mean=589.0 | loss=17.846 pg=0.057 v=35.590 ent=0.586 | clipfrac=0.091 exp_var=0.597\n",
      "Update 141/152 | step=9240576 | ep_ret_mean=184.8 ep_len_mean=612.7 | loss=19.353 pg=-0.009 v=38.736 ent=0.610 | clipfrac=0.078 exp_var=0.543\n",
      "Update 142/152 | step=9306112 | ep_ret_mean=199.5 ep_len_mean=557.7 | loss=29.833 pg=-0.041 v=59.760 ent=0.599 | clipfrac=0.068 exp_var=0.557\n",
      "Update 143/152 | step=9371648 | ep_ret_mean=192.5 ep_len_mean=598.1 | loss=9.352 pg=0.051 v=18.614 ent=0.617 | clipfrac=0.056 exp_var=0.654\n",
      "Update 144/152 | step=9437184 | ep_ret_mean=186.3 ep_len_mean=577.8 | loss=17.400 pg=-0.009 v=34.831 ent=0.591 | clipfrac=0.045 exp_var=0.672\n",
      "Update 145/152 | step=9502720 | ep_ret_mean=198.1 ep_len_mean=559.8 | loss=22.576 pg=-0.039 v=45.242 ent=0.599 | clipfrac=0.037 exp_var=0.706\n",
      "Update 146/152 | step=9568256 | ep_ret_mean=186.5 ep_len_mean=551.0 | loss=14.681 pg=0.005 v=29.363 ent=0.592 | clipfrac=0.050 exp_var=0.686\n",
      "Update 147/152 | step=9633792 | ep_ret_mean=195.7 ep_len_mean=527.7 | loss=18.930 pg=0.045 v=37.782 ent=0.593 | clipfrac=0.045 exp_var=0.673\n",
      "Update 148/152 | step=9699328 | ep_ret_mean=202.0 ep_len_mean=542.5 | loss=15.694 pg=-0.056 v=31.511 ent=0.549 | clipfrac=0.036 exp_var=0.765\n",
      "Update 149/152 | step=9764864 | ep_ret_mean=193.0 ep_len_mean=557.7 | loss=21.974 pg=-0.118 v=44.196 ent=0.567 | clipfrac=0.043 exp_var=0.734\n",
      "Update 150/152 | step=9830400 | ep_ret_mean=205.0 ep_len_mean=571.5 | loss=16.887 pg=-0.072 v=33.930 ent=0.576 | clipfrac=0.030 exp_var=0.753\n",
      "Update 151/152 | step=9895936 | ep_ret_mean=205.5 ep_len_mean=503.5 | loss=13.636 pg=-0.002 v=27.289 ent=0.583 | clipfrac=0.040 exp_var=0.751\n",
      "Update 152/152 | step=9961472 | ep_ret_mean=210.6 ep_len_mean=497.3 | loss=15.643 pg=0.011 v=31.275 ent=0.566 | clipfrac=0.048 exp_var=0.746\n",
      "Training Completed. Model saved to: models/exp_1_final.pth\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "# learning_rate, num_steps, total_timesteps, num_envs, batch_size, minibatch_size, update_epochs\n",
    "# gamma, gae_lambda, eps, ent_coef, v_loss_coef, obs_dim, device\n",
    "# rollout for buffer data\n",
    "learning_rate = 2.5e-4\n",
    "num_steps = 2048 # sample steps per env\n",
    "total_timesteps = 10000000 # total training steps\n",
    "batch_size = int(num_envs * num_steps)\n",
    "minibatch_size = 512 # batch for every grad update\n",
    "update_epochs = 4\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "eps = 0.2\n",
    "ent_coef = 0.01\n",
    "v_loss_coef = 0.5\n",
    "obs_dim = envs.single_observation_space.shape[0]\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# define buffer\n",
    "obs = torch.zeros([num_steps, num_envs, obs_dim], dtype=torch.float32, device=device)\n",
    "# why we don't store actions and log_probs like [num_steps, num_envs, envs.single_action_space.n]?\n",
    "# because for ppo's grad backward update, we only use the selected action and can't reselect another new action\n",
    "actions = torch.zeros([num_steps, num_envs], dtype=torch.long, device=device)\n",
    "log_probs = torch.zeros([num_steps, num_envs], dtype=torch.float32, device=device)\n",
    "rewards = torch.zeros([num_steps, num_envs], dtype=torch.float32, device=device)\n",
    "dones = torch.zeros([num_steps, num_envs], dtype=torch.float32, device=device)\n",
    "values = torch.zeros([num_steps, num_envs], dtype=torch.float32, device=device)\n",
    "\n",
    "# Initialize Agent and Optimizer\n",
    "agent = LunarLanderMLP(envs).to(device)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=learning_rate, eps=1e-5)\n",
    "\n",
    "# Initialize environment state\n",
    "next_obs_np, _ = envs.reset(seed=42)\n",
    "next_obs_tensor = torch.as_tensor(next_obs_np, dtype=torch.float32, device=device)\n",
    "next_done = torch.zeros([num_envs], dtype=torch.float32, device=device)\n",
    "\n",
    "num_updates = total_timesteps // batch_size\n",
    "global_step = 0\n",
    "\n",
    "print(\"Start Training...\")\n",
    "\n",
    "for update in range(1, num_updates + 1):\n",
    "    ep_returns, ep_lengths = [], []\n",
    "\n",
    "    # rollout\n",
    "    for t in range(num_steps):\n",
    "        global_step += num_envs\n",
    "        with torch.no_grad():\n",
    "            action_tensor, log_prob_tensor, entropy_tensor, value_tensor = agent.get_action_and_value(next_obs_tensor)\n",
    "\n",
    "        obs[t] = next_obs_tensor\n",
    "        actions[t] = action_tensor\n",
    "        log_probs[t] = log_prob_tensor\n",
    "        values[t] = value_tensor\n",
    "\n",
    "        action_np = action_tensor.cpu().numpy()\n",
    "        next_obs_np, reward, terminated, truncated, info = envs.step(action_np)\n",
    "        \n",
    "        # Handle episodic info\n",
    "        if isinstance(info, dict) and \"episode\" in info:\n",
    "            ep = info[\"episode\"]\n",
    "            done_mask = ep.get(\"_r\", info.get(\"_episode\", None))\n",
    "            if done_mask is not None:\n",
    "                for i in np.where(done_mask)[0]:\n",
    "                    ep_returns.append(float(ep[\"r\"][i]))\n",
    "                    ep_lengths.append(int(ep[\"l\"][i]))\n",
    "                    \n",
    "        next_obs_tensor = torch.as_tensor(next_obs_np, dtype=torch.float32, device=device)\n",
    "        done_np = np.logical_or(terminated, truncated)\n",
    "\n",
    "        rewards[t] = torch.as_tensor(reward, dtype=torch.float32, device=device)\n",
    "        dones[t] = torch.as_tensor(done_np, dtype=torch.float32, device=device)\n",
    "        next_done = dones[t]\n",
    "\n",
    "        # episodic stats (alternative location)\n",
    "        if isinstance(info, dict) and \"final_info\" in info:\n",
    "            for finfo in info[\"final_info\"]:\n",
    "                if finfo and \"episode\" in finfo:\n",
    "                    ep_returns.append(float(finfo[\"episode\"][\"r\"]))\n",
    "                    ep_lengths.append(int(finfo[\"episode\"][\"l\"]))\n",
    "\n",
    "    # GAE\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs_tensor).squeeze(-1)\n",
    "    advantages, returns = compute_gae(\n",
    "        rewards=rewards, dones=dones, values=values,\n",
    "        next_value=next_value, next_done=next_done,\n",
    "        gamma=gamma, gae_lambda=gae_lambda\n",
    "    )\n",
    "\n",
    "    # flatten\n",
    "    obs_batch = obs.reshape(-1, obs_dim)\n",
    "    actions_batch = actions.reshape(-1)\n",
    "    log_probs_batch = log_probs.reshape(-1)\n",
    "    advantages_batch = advantages.reshape(-1)\n",
    "    returns_batch = returns.reshape(-1)\n",
    "    values_batch = values.reshape(-1)\n",
    "\n",
    "    # normalize advantages once per update\n",
    "    advantages_batch = (advantages_batch - advantages_batch.mean()) / (advantages_batch.std() + 1e-8)\n",
    "\n",
    "    # PPO update\n",
    "    clipfracs = []\n",
    "    last_pg_loss = last_v_loss = last_ent = None\n",
    "\n",
    "    for epoch in range(update_epochs):\n",
    "        b_inds = torch.randperm(batch_size, device=device)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            mb_inds = b_inds[start:start + minibatch_size]\n",
    "\n",
    "            _, new_log_prob, entropy, new_value = agent.get_action_and_value(\n",
    "                obs_batch[mb_inds],\n",
    "                action=actions_batch[mb_inds]\n",
    "            )\n",
    "\n",
    "            log_ratio = new_log_prob - log_probs_batch[mb_inds]\n",
    "            ratio = log_ratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                clipfracs.append(((ratio - 1.0).abs() > eps).float().mean().item())\n",
    "\n",
    "            mb_adv = advantages_batch[mb_inds]\n",
    "            pg_loss1 = -mb_adv * ratio\n",
    "            pg_loss2 = -mb_adv * torch.clamp(ratio, 1 - eps, 1 + eps)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            v_loss = 0.5 * (new_value - returns_batch[mb_inds]).pow(2).mean()\n",
    "\n",
    "            ent = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * ent + v_loss_coef * v_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            last_pg_loss, last_v_loss, last_ent = pg_loss, v_loss, ent\n",
    "\n",
    "    # explained variance (per update)\n",
    "    y_pred = values_batch.detach().cpu().numpy()\n",
    "    y_true = returns_batch.detach().cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # print once per update\n",
    "    if len(ep_returns) > 0:\n",
    "        print(\n",
    "            f\"Update {update}/{num_updates} | step={global_step} | \"\n",
    "            f\"ep_ret_mean={np.mean(ep_returns):.1f} ep_len_mean={np.mean(ep_lengths):.1f} | \"\n",
    "            f\"loss={float(loss.detach().cpu()):.3f} pg={float(last_pg_loss.detach().cpu()):.3f} \"\n",
    "            f\"v={float(last_v_loss.detach().cpu()):.3f} ent={float(last_ent.detach().cpu()):.3f} | \"\n",
    "            f\"clipfrac={np.mean(clipfracs):.3f} exp_var={explained_var:.3f}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Update {update}/{num_updates} | step={global_step} | \"\n",
    "            f\"loss={float(loss.detach().cpu()):.3f} pg={float(last_pg_loss.detach().cpu()):.3f} \"\n",
    "            f\"v={float(last_v_loss.detach().cpu()):.3f} ent={float(last_ent.detach().cpu()):.3f} | \"\n",
    "            f\"clipfrac={np.mean(clipfracs):.3f} exp_var={explained_var:.3f}\"\n",
    "        )\n",
    "\n",
    "# save model params\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_path = f\"models/{run_name}_final.pth\"\n",
    "torch.save(agent.state_dict(), model_path)\n",
    "print(f\"Training Completed. Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "772f8086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation and Recording...\n",
      "final eval return: 214.93616379451146 video saved to: videos/exp_1-final\n"
     ]
    }
   ],
   "source": [
    "# Evaluation and Video Recording\n",
    "print(\"Starting Evaluation and Recording...\")\n",
    "\n",
    "video_dir = f\"videos/{run_name}-final\"\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Create evaluation environment (Evaluation needs only 1 env)\n",
    "# Set max_episode_steps to avoid early truncation during recording\n",
    "eval_env = gym.make(env_id, render_mode=\"rgb_array\", max_episode_steps=5000)\n",
    "eval_env = gym.wrappers.RecordVideo(\n",
    "    eval_env,\n",
    "    video_folder=video_dir,\n",
    "    episode_trigger=lambda episode_id: True, # Record all episodes (we run only 1 here)\n",
    "    name_prefix=\"final\",\n",
    ")\n",
    "eval_env = gym.wrappers.RecordEpisodeStatistics(eval_env)\n",
    "\n",
    "obs, _ = eval_env.reset(seed=123)\n",
    "done = False\n",
    "ep_ret = 0.0\n",
    "\n",
    "while not done:\n",
    "    obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)  # [1, obs_dim]\n",
    "    with torch.no_grad():\n",
    "        logits = agent.actor(obs_t)\n",
    "        action = int(torch.argmax(logits, dim=-1).item())  # use argmax for deterministic\n",
    "\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "    done = bool(terminated or truncated)\n",
    "    ep_ret += float(reward)\n",
    "\n",
    "eval_env.close()\n",
    "print(\"final eval return:\", ep_ret, \"video saved to:\", video_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
