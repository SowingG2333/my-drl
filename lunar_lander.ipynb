{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc51ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mlp network for lunarlander\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def layer_init(layer: nn.Module, std: float, bias: float) -> nn.Module:\n",
    "    # initialize the linear/convolutional layer\n",
    "    if hasattr(layer, \"weight\") and layer.weight is not None:\n",
    "        nn.init.orthogonal_(layer.weight, gain=std)\n",
    "    if hasattr(layer, \"bias\") and layer.bias is not None:\n",
    "        nn.init.constant_(layer.bias, bias)\n",
    "    return layer\n",
    "\n",
    "class LunarLanderMLP(nn.Module):\n",
    "    # initialize the mlp\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        # define the critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(8, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0)\n",
    "        )\n",
    "        # define the actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(8, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.action_space.n), std=0.01)\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x) # [batch, num_actions]\n",
    "        dist = Categorical(logits=logits) # [batch, num_actions] after softmax to construct a distribution\n",
    "        if action is None: # output actions in sample stage\n",
    "            action = dist.sample() # tips: tensor.sample() is wrong!!!\n",
    "        log_prob = dist.log_prob(action) # [batch]\n",
    "        entropy = dist.entropy() # [batch]\n",
    "        value = self.critic(x).squeeze(-1) # [batch]\n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a81b431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_gae(\n",
    "    rewards: torch.Tensor, \n",
    "    dones: torch.Tensor, \n",
    "    values: torch.Tensor, \n",
    "    next_value: torch.Tensor, \n",
    "    next_done: torch.Tensor, \n",
    "    gamma: float, \n",
    "    gae_lambda: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # calculate GAE: general advantage estimination\n",
    "    # need: next_obs(next_value, next_done), rewards, dones, values\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    next_advantage = 0\n",
    "    num_steps = rewards.shape[0]\n",
    "    for t in reversed(range(num_steps)):\n",
    "        # if num_step == 100, t = 99, 98 ... 0\n",
    "        # if done == 0, mask == 1, game didn't stop\n",
    "        # next_done and next_value is the current state (internal vars)\n",
    "        # check if it is the last step\n",
    "        if t == num_steps - 1:\n",
    "            mask = 1.0 - next_done\n",
    "            next_value = next_value \n",
    "        else:\n",
    "            # if it is not the last step, check the next step in the buffer\n",
    "            mask = 1.0 - dones[t + 1]\n",
    "            next_value = values[t + 1]\n",
    "        # calculate the td_error/delta\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        # calculate the GAE\n",
    "        current_advantage = delta + gamma * gae_lambda * mask * next_advantage\n",
    "        advantages[t] = current_advantage # save current advantage\n",
    "        next_advantage = current_advantage\n",
    "    returns = advantages + values\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8fe23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LunarLander envs are created! obs_shape:(8,); action_n:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sowingg/miniconda3/envs/drl/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "# define hyper-params\n",
    "env_id = \"LunarLander-v3\"\n",
    "num_envs = 16\n",
    "capture_video = False\n",
    "\n",
    "# environment factory function\n",
    "def make_env(env_id: str, idx: int, capture_video: bool, run_name: str) -> Callable[[], gym.Env]:\n",
    "    def thunk() -> gym.Env:\n",
    "        # create the basic env\n",
    "        if capture_video and idx == 0: # only capture the env_id == 0\n",
    "            env = gym.make(env_id, render_mode='rgb_array')\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        # add a wrapper to get the real rewards\n",
    "        # what is a wrapper in gym?\n",
    "        # wrapper can add functions for the env without modifying the source code\n",
    "        # e.g., output more infos after env.step(action) or record videos\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "# create the vector envs\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, i, capture_video, \"exp_1\") for i in range(num_envs)]\n",
    ")   \n",
    "\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete)\n",
    "print(\n",
    "    f\"LunarLander envs are created! \"\n",
    "    f\"obs_shape:{envs.single_observation_space.shape}; \"\n",
    "    f\"action_n:{envs.single_action_space.n}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
